{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from math import log\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    '''\n",
    "    Helper function to check if value is numeric or not\n",
    "    '''\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplittingCriteria:\n",
    "    def __init__(self, column, value):\n",
    "        '''\n",
    "        This class initializes the column name and the value at each node where the split occurs\n",
    "        Eg - If questions is 'is sepal_length >= 1cm', then col=sepal_length and value=1cm\n",
    "        '''\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, data):\n",
    "        '''\n",
    "        This function checks if the supplied data meets the splitting criteria posed by the Question\n",
    "        @returns - True if criteria is met, False otherwise\n",
    "        '''\n",
    "        value = data[self.column]\n",
    "        if is_numeric(value):\n",
    "            return value >= self.value\n",
    "        else:\n",
    "            return value == self.value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        This function prints the Question in a readable format\n",
    "        '''\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is {column} {condition} {value}?\".format(column=self.column, condition=condition, value=self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, split, true_branch, false_branch):\n",
    "        '''\n",
    "        This class represents the internal node structure of Decision Tree\n",
    "        @params - split: Stores column and value variables regarding the splitting criteria of that node\n",
    "                  true_branch: points to the true branch after splitting\n",
    "                  false_branch: points to the false branch after splitting\n",
    "        '''\n",
    "        self.split = split\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionLeaf:\n",
    "    def __init__(self, predictions):\n",
    "        '''\n",
    "        This class represents the leaf node structure of the Decision Tree\n",
    "        @params - predictions: A dictionary that stores the counts of occurences of the different classes\n",
    "        '''\n",
    "        self.predictions = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini'):\n",
    "        '''\n",
    "        This class represents a Decision tree along with the required methods\n",
    "        @params - criterion: Criteria to be used to measure quality of split.\n",
    "                             Supported criteria are 'gini' for gini impurity and \n",
    "                             'entropy' for information gain.\n",
    "        '''\n",
    "        \n",
    "        def gini(dataset):\n",
    "            '''\n",
    "            This function calculates the Gini impurity of the given dataset\n",
    "            @params - dataset: The dataset whose impurity is to be calculated\n",
    "            @returns - impurity: The impurity value of the data\n",
    "            '''\n",
    "            count = self.label_counts(dataset)\n",
    "            impurity = 1\n",
    "            for label in count:\n",
    "                probability_of_label = count[label] / float(len(dataset))\n",
    "                impurity -= probability_of_label**2\n",
    "            return impurity\n",
    "    \n",
    "        def entropy(dataset):\n",
    "            '''\n",
    "            This function calculates the Entropy of the given dataset\n",
    "            @params - dataset: The dataset whose entropy is to be calculated\n",
    "            @returns - entropy: The entropy value for the given dataset\n",
    "            '''\n",
    "            count = self.label_counts(dataset)\n",
    "            entropy = 0\n",
    "            for label in count:\n",
    "                probability_of_label = count[label] / float(len(dataset))\n",
    "                entropy -= probability_of_label * log(probability_of_label, 2)\n",
    "            return entropy\n",
    "        \n",
    "        criteria = {'gini':gini, 'entropy':entropy}\n",
    "        self.evaluation_function = criteria.get(criterion, gini)\n",
    "\n",
    "    def partition(self, dataset, splitting_criteria):\n",
    "        '''\n",
    "        This function partitions the dataset based on the given splitting criteria\n",
    "        @params - dataset: The dataset to be split\n",
    "                  splitting_criteria: The column and value on which the split needs to be performed\n",
    "        @returns - true_dataset: The records in the dataset that satisfy the criteria\n",
    "                   false_dataset: The records in the dataset that do not satisfy the criteria\n",
    "        '''\n",
    "        true_dataset, false_dataset = [], []\n",
    "        for row in dataset:\n",
    "            if splitting_criteria.match(row):\n",
    "                true_dataset.append(row)\n",
    "            else:\n",
    "                false_dataset.append(row)\n",
    "        return true_dataset, false_dataset\n",
    "    \n",
    "    def label_counts(self, dataset):\n",
    "        '''\n",
    "        This function that counts the number of instances for each label in the dataset\n",
    "        @params - row_labels: The labels for each row in the dataset\n",
    "        @returns - count: A dictionary where keys are the different labels and the corresponding value\n",
    "                          is the frequency of occurence of the label\n",
    "        '''\n",
    "        count={}\n",
    "        #takes whole dataset in as argument\n",
    "        for row in dataset:\n",
    "            #traverse on each datapoint\n",
    "            label=row[-1]\n",
    "            #labels are in the last column\n",
    "            #if label is not even once come initialise it\n",
    "            if label not in count:\n",
    "                count[label]=0\n",
    "            #increase the count of present label by 1\n",
    "            count[label]+=1\n",
    "        return count \n",
    "    \n",
    "    def information_gain(self, current, left, right):\n",
    "        '''\n",
    "        This function calculates the information gain using the evaluation function\n",
    "        @params - current: The gini impurity or entropy value of the current full dataset\n",
    "                  left: The true branch after splitting\n",
    "                  right: The false branch after splitting\n",
    "        @returns - info_gain: The information_gain value at the current node\n",
    "        '''\n",
    "        p = float(len(left) / (len(left) + len(right)))\n",
    "        info_gain = current - p * self.evaluation_function(left) - (1 - p) * self.evaluation_function(right)\n",
    "        return info_gain\n",
    "    \n",
    "    def find_best_split(self, dataset):\n",
    "        '''\n",
    "        This function finds the column that provides the best information gain and the splitting value\n",
    "        @params - dataset: The dataset in which the best split needs to be found\n",
    "        @returns - best_gain: The best gain value using the current dataset\n",
    "                   splitting_criteria: The splitting criteria that contains the splitting column and value\n",
    "        '''\n",
    "        best_gain = 0\n",
    "        best_splitting_criteria = None\n",
    "        # Calculate the current gain\n",
    "        current_gain = self.evaluation_function(dataset)\n",
    "        # The total number of features without the target\n",
    "        n_features = len(dataset[0]) - 1\n",
    "        for col in range(n_features):\n",
    "            # Set of all unique values in a single feature\n",
    "            feature_values = set([row[col] for row in dataset])\n",
    "            # Iterate through each unique class and try to split using that value\n",
    "            for value in feature_values:\n",
    "                # Create the splitting criteria\n",
    "                splitting_criteria = SplittingCriteria(col, value)\n",
    "                # Devide the dataset based on the criteria\n",
    "                true_dataset, false_dataset = self.partition(dataset, splitting_criteria)\n",
    "                # No splitting occurs, let us go to the next iteration\n",
    "                if len(true_dataset) == 0 or len(false_dataset) == 0:\n",
    "                    continue\n",
    "                info_gain = self.information_gain(current_gain, true_dataset, false_dataset)\n",
    "                # Check if it is more than the best gain yet\n",
    "                if info_gain > best_gain:\n",
    "                    best_gain, best_splitting_criteria = info_gain, splitting_criteria\n",
    "        return best_gain, best_splitting_criteria\n",
    "    \n",
    "    def build_tree(self, dataset):\n",
    "        '''\n",
    "        This function builds the actual Decision Tree\n",
    "        @params - dataset: The training dataset\n",
    "        @returns - root_node: The root node of the Decision tree\n",
    "        '''\n",
    "        # take the whole dataset and find the initial best split\n",
    "        info_gain, splitting_criteria = self.find_best_split(dataset)\n",
    "        # If info_gain is zero, leaf node condition is satisfied\n",
    "        if info_gain == 0:\n",
    "            return DecisionLeaf(self.label_counts(dataset))\n",
    "        # Else, we have found the best splitting criteria\n",
    "        true_dataset, false_dataset = self.partition(dataset, splitting_criteria)\n",
    "        # Recursively build the tree for the true and false branches\n",
    "        true_branch = self.build_tree(true_dataset)\n",
    "        false_branch = self.build_tree(false_dataset)\n",
    "        # Return the root node of the tree\n",
    "        return DecisionNode(splitting_criteria, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion='gini'):\n",
    "        '''\n",
    "        This class creates a classifier using the Decision Tree class\n",
    "        @params - criterion: Criteria to be used to measure quality of split.\n",
    "                             Supported criteria are 'gini' for gini impurity and \n",
    "                             'entropy' for information gain.\n",
    "        '''\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        This function uses the given dataset and creates a decision tree\n",
    "        @params - X: Training data\n",
    "                  y: Labels of training data\n",
    "        '''\n",
    "        dataset = np.c_[X, y]\n",
    "        dt = DecisionTree(criterion=self.criterion)\n",
    "        self.tree = dt.build_tree(dataset)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        This function predicts the labels of the given data\n",
    "        @params - X: Data for which labels have to be predicted\n",
    "        @returns - y: List of predicted labels\n",
    "        '''\n",
    "        if self.tree is None:\n",
    "            print(\"Please fit the dataset using the fit function before prediciting values\")\n",
    "            return\n",
    "        y = []\n",
    "        for row in X:\n",
    "            y.append(self.predictInstance(row))\n",
    "        return y\n",
    "    \n",
    "    def predictInstance(self, row):\n",
    "        '''\n",
    "        This function recursively iterates through the decision tree and predicts the label\n",
    "        of a single row of data\n",
    "        @params - row: A single row of data\n",
    "        @returns - row_pred: The label predicted by the decision tree for this data\n",
    "        '''\n",
    "        dt_node = self.tree\n",
    "        while type(dt_node) is not DecisionLeaf:\n",
    "            splitting_criteria = dt_node.split\n",
    "            if splitting_criteria.match(row):\n",
    "                dt_node = dt_node.true_branch\n",
    "            else:\n",
    "                dt_node = dt_node.false_branch\n",
    "        return list(dt_node.predictions.keys())[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 31}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{0.0: 31}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{0.0: 31}\n",
      "{0.0: 31}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n",
      "{0.0: 31}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n",
      "{1.0: 27}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{1.0: 27}\n",
      "{0.0: 31}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{0.0: 31}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n",
      "{0.0: 31}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{2.0: 25}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{2.0: 25}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{0.0: 31}\n",
      "{0.0: 31}\n",
      "{0.0: 31}\n",
      "{1.0: 27}\n",
      "{0.0: 31}\n",
      "{0.0: 31}\n",
      "{2.0: 25}\n",
      "{2.0: 25}\n",
      "{2.0: 25}\n",
      "{2.0: 25}\n",
      "{2.0: 2}\n",
      "{1.0: 27}\n",
      "{2.0: 25}\n",
      "{1.0: 27}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=1)\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       2, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 1, 2,\n",
       "       1, 2, 0, 0, 0, 1, 0, 0, 2, 2, 2, 2, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train).predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
